<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
  <head>
    <link rel="shortcut icon" type="image/x-icon" href="images/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>Yue Gao</title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" />
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css" />

    <link rel="stylesheet" type="text/css" href="static/css/style.css" />

    <script>
      // Check for system dark mode preference
      const prefersDarkScheme = window.matchMedia("(prefers-color-scheme: dark)");

      // Set initial theme based on system preference
      if (prefersDarkScheme.matches) {
        document.documentElement.setAttribute("data-theme", "dark");
      }

      // Listen for system theme changes
      prefersDarkScheme.addEventListener("change", (e) => {
        document.documentElement.setAttribute("data-theme", e.matches ? "dark" : "light");
      });
    </script>
  </head>

  <body>
    <div class="researcher">
      <p class="bigname-top">Yue Gao</p>
      <table align="center" border="0" cellpadding="0">
        <tr>
          <td class="avatar-cell-top">
            <div class="researcherphoto">
              <img src="images/yue.png" id="yue" />
            </div>
          </td>
        </tr>
        <tr>
          <td class="bio-cell">
            <p class="bigname">Yue Gao</p>
            <!-- <p align="center">
              <a href="mailto:yuegao@cs.stanford.edu" class="breakable-email code-font">yuegao@cs.stanford.edu</a>
            </p> -->
            <p align="center" class="code-font">yuegao@cs.stanford.edu</p>

            <p>My name is pronounced as <a href="https://www.howtopronounce.com/yue">yoo-eh</a>.</p>

            <p align="justify">
              I am an incoming PhD student at the <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a> in the
              <a href="https://www.cs.stanford.edu/">Computer Science</a> department of <a href="https://www.stanford.edu/">Stanford University</a>.
              I've had a wonderful experience being advised by <a href="https://jiajunwu.com/"><span class="nowrap">Prof. Jiajun Wu</span></a> and
              <a href="https://faculty.cc.gatech.edu/~bozhu/"><span class="nowrap">Prof. Bo Zhu</span></a>, and collaborating with <a href="https://kovenyu.com/"><span class="nowrap">Koven Yu</span></a>.
            </p>

            <p align="justify">
              Before joining Stanford, I was a researcher at <a href="https://www.microsoft.com/en-us/research/people/yuegao/">Microsoft Research</a>,
              focusing on video generation and editing. I earned my M.S. with distinction in research from
              <a href="https://www.pku.edu.cn">Peking University</a>, under the supervision of
              <a href="https://www.wict.pku.edu.cn/zlian/"><span class="nowrap">Prof. Zhouhui Lian</span></a>.
            </p>

            <p class="social-links">
              <a href="https://github.com/ueoo" target="_blank">
                <img src="images/github.svg" height="26" />
              </a>
              <a href="https://scholar.google.com/citations?user=XPB9hGwAAAAJ" target="_blank">
                <img src="images/google-scholar.svg" height="26" />
              </a>
              <a href="https://www.linkedin.com/in/yuegaocs/" target="_blank">
                <img src="images/linkedin.svg" height="26" />
              </a>
              <a href="https://x.com/yuegao_cs" target="_blank">
                <img src="images/x.svg" height="26" />
              </a>
            </p>
          </td>
          <td class="avatar-cell">
            <div class="researcherphoto">
              <img src="images/yue.png" id="yue" />
            </div>
          </td>
        </tr>
      </table>
    </div>
    <br />

    <div class="research-interests">
      <h2 class="research-title">
        <span class="color1">Dynamic Physical World</span> <span class="color2">Understanding</span> and <span class="color3">Prediction</span>
      </h2>
      <div class="research-desc">
        My primary research interests lie in <b>computer vision</b> and <b>computer graphics</b>. Specifically, I aim to enable computers to
        understand and predict the physical world by integrating advanced algorithms, such as perception and generative models, with foundational
        knowledge from physics.
      </div>
    </div>
    <br />

    <div class="publication">
      <h2>Publications</h2>

      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <video muted loop autoplay playsinline webkit-playsinline style="width: 100%; height: auto; display: block">
              <source src="teasers/FluidNexus.mp4" type="video/mp4" />
            </video>
          </td>
          <td class="info-cell">
            <p>
              <a href="https://arxiv.org/abs/2503.04720">
                <b>FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video</b>
              </a>
              <br />
              <b>Yue Gao*</b>, <a href="https://kovenyu.com">Hong-Xing Yu*</a>, <a href="https://faculty.cc.gatech.edu/~bozhu">Bo Zhu</a>,
              <a href="https://jiajunwu.com">Jiajun Wu</a><br />
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2025<br />
              <b class="highlight">Oral Presentation</b>
            </p>
            <a href="https://arxiv.org/pdf/2503.04720">pdf</a> / <a href="https://yuegao.me/FluidNexus">website</a> /
            <a href="https://github.com/ueoo/FluidNexus">code</a>
          </td>
        </table>
        <hr />
      </div>

      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <img src="teasers/IMF.jpeg" alt="IMF" style="border-style: none" />
          </td>
          <td class="info-cell">
            <p>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Implicit_Motion_Function_CVPR_2024_paper.pdf">
                <b>Implicit Motion Function</b> </a
              ><br />
              <b>Yue Gao</b>, <a href="https://scholar.google.com/citations?user=AcOcw0AAAAAJ">Jiahao Li</a>,
              <a href="https://lei65537.github.io/">Lei Chu</a>,
              <a href="https://www.microsoft.com/en-us/research/people/yanlu/">Yan Lu</a>
              <br /><em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024<br />
            </p>
            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Implicit_Motion_Function_CVPR_2024_paper.pdf">pdf</a>
            /
            <a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Gao_Implicit_Motion_Function_CVPR_2024_supplemental.pdf">supp</a>
            /
            <a href="https://yuegao.me/IMF">website</a>
          </td>
        </table>
        <hr />
      </div>
      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <img src="teasers/PECHead.jpeg" alt="PECHead" style="border-style: none" />
          </td>
          <td class="info-cell">
            <p>
              <a href="https://arxiv.org/pdf/2304.10168.pdf"><b>High-Fidelity and Freely Controllable Talking Head Video Generation</b></a>
              <br />
              <b>Yue Gao</b>, <a href="https://www.microsoft.com/en-us/research/people/zhouyuan/">Yuan Zhou</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jinglwa/">Jinglu Wang</a>, <a href="https://pableeto.github.io/">Xiao Li</a>,
              <a href="https://www.microsoft.com/en-us/research/people/xiangming/">Xiang Ming</a>,
              <a href="https://www.microsoft.com/en-us/research/people/yanlu/">Yan Lu</a>
              <br /><em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023<br />
            </p>
            <a href="https://arxiv.org/abs/2304.10168">arXiv</a> /
            <a
              href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2023_paper.pdf"
              >pdf</a
            >
            /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Gao_High-Fidelity_and_Freely_CVPR_2023_supplemental.pdf">supp</a>
            /
            <a href="https://yuegao.me/PECHead">website</a>
          </td>
        </table>
        <hr />
      </div>
      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <img src="teasers/DetPro.png" alt="DetPro" style="border-style: none" />
          </td>
          <td class="info-cell">
            <p>
              <a href="https://arxiv.org/pdf/2203.14940.pdf"
                ><b>Learning to Prompt for Open-Vocabulary Object Detection With Vision-Language Model</b></a
              >
              <br />
              Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, <b>Yue Gao</b>, Guoqi Li. <br />
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2022
              <br />
            </p>
            <a href="https://arxiv.org/abs/2203.14940">arXiv</a> /
            <a href="https://github.com/dyabel/detpro">code</a>
          </td>
        </table>
        <hr />
      </div>
      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <img src="teasers/SoCo.png" alt="SoCo" style="border-style: none" />
          </td>
          <td class="info-cell">
            <p>
              <a href="https://arxiv.org/pdf/2106.02637.pdf">
                <b>Aligning Pretraining for Detection via Object-Level Contrastive Learning</b>
              </a>
              <br />
              <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJn">Fangyun Wei*</a>, <b>Yue Gao*</b>,
              <a href="https://scholar.google.com/citations?user=lH4zgcIAAAAJ">Zhirong Wu</a>, <a href="https://ancientmooner.github.io">Han Hu</a>,
              <a href="https://www.microsoft.com/en-us/research/people/stevelin">Stephen Lin</a><br />
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021<br />
              <b class="highlight">Spotlight</b>
            </p>
            <a href="https://arxiv.org/abs/2106.02637">arXiv</a> / <a href="https://github.com/ueoo/SoCo/">code</a>
          </td>
        </table>
        <hr />
      </div>
      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <img src="teasers/HifaFace.png" alt="HifaFace" style="border-style: none" />
          </td>
          <td class="info-cell">
            <p>
              <a href="https://arxiv.org/pdf/2103.15814.pdf"><b>High-Fidelity and Arbitrary Face Editing</b></a
              ><br />
              <b>Yue Gao</b>, <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJn">Fangyun Wei</a>,
              <a href="https://jianminbao.github.io/">Jianmin Bao</a>,
              <a href="https://https://scholar.google.com/citations?hl=en&user=-NnIabUAAAAJ/">Shuyang Gu</a>,
              <a href="http://dongchen.pro">Dong Chen</a>, <a href="https://www.microsoft.com/en-us/research/people/fangwen/"> Fang Wen</a>,
              <a href="https://www.wict.pku.edu.cn/zlian/"><span class="nowrap">Prof. Zhouhui Lian</span></a><br />
              († Corresponding author)<br />
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2021<br />
            </p>
            <a href="https://arxiv.org/abs/2103.15814">arXiv</a> / <a href="https://yuegao.me/HifaFace">website</a>
          </td>
        </table>
        <hr />
      </div>
      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <img src="teasers/Attr2Font.png" alt="Attr2Font" style="border-style: none" />
          </td>
          <td class="info-cell">
            <p>
              <a href="https://arxiv.org/pdf/2005.07865.pdf"><b>Attribute2Font: Creating Fonts You Want From Attributes</b></a
              ><br />
              <a href="https://actasidiot.github.io/">Yizhi Wang*</a>, <b>Yue Gao*</b>, <a href="https://www.wict.pku.edu.cn/zlian/"><span class="nowrap">Prof. Zhouhui Lian</span></a
              ><br />
              <em>SIGGRAPH, ACM Transactions on Graphics (TOG)</em>, 2020 <br />
            </p>

            <a href="https://arxiv.org/abs/2005.07865">arXiv</a> / <a href="https://yuegao.me/Attr2Font">code</a>
          </td>
        </table>
        <hr />
      </div>
      <div class="paper">
        <table align="center" border="0" cellpadding="0">
          <td class=".image-cell">
            <img src="teasers/AGIS-Net.png" alt="AGIS-Net" style="border-style: none" />
          </td>
          <td class="info-cell">
            <p>
              <a href="https://arxiv.org/pdf/1910.04987.pdf"><b>Artistic Glyph Image Synthesis via One-Stage Few-Shot Learning</b></a
              ><br />
              <b>Yue Gao*</b>, Yuan Guo*, <a href="https://www.wict.pku.edu.cn/zlian/"><span class="nowrap">Prof. Zhouhui Lian</span></a>, Yingmin Tang, Jianguo Xiao<br />
              <em>SIGGRAPH Asia, ACM Transactions on Graphics (TOG)</em>, 2019<br />
            </p>
            <a href="https://arxiv.org/abs/1910.04987">arXiv</a> / <a href="https://yuegao.me/AGIS-Net">code</a>
          </td>
        </table>
      </div>
    </div>
    <br />

    <div class="container">
      <h2>Education</h2>
      <div class="experience">
        <table align="center" border="0" cellpadding="0">
          <td class="image-cell">
            <img src="images/stanford.png" alt="stanford" />
          </td>
          <td class="info-cell">
            <h4>
              <a href="https://www.stanford.edu/">Stanford University</a>
            </h4>
            <p><b>Ph.D.</b> in Computer Science</p>
            <p><a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a></p>
            <p>
              <em>Sep. 2025 - Present</em>
            </p>
          </td>
        </table>
        <hr />
      </div>
      <div class="experience">
        <table align="center" border="0" cellpadding="0">
          <td class="image-cell">
            <img src="images/pku.png" alt="pku" />
          </td>
          <td class="info-cell">
            <h4>
              <a href="https://www.pku.edu.cn">Peking University</a>
            </h4>
            <p><b>Master of Natural Science</b> in Computer Applied Technology</p>
            <p>
              Awards:<br />
              <span class="award-item">
                <b>Excellent Graduate</b>
                <br />
                <b>Merit Student</b>
                <br />
                <b>Outstanding Student of Wangxuan Institute of Computer Technology</b>
              </span>
            </p>
            <p>
              Advisor: <a href="https://www.wict.pku.edu.cn/zlian/"><span class="nowrap">Prof. Zhouhui Lian</span></a>
            </p>
            <p>
              <em>Sep. 2018 - Jun. 2021</em>
            </p>
          </td>
        </table>
      </div>
    </div>
    <br />

    <div class="container">
      <h2>Work</h2>

      <div class="experience">
        <table align="center" border="0" cellpadding="0">
          <td class="image-cell">
            <img src="images/microsoft.svg" alt="microsoft" />
          </td>
          <td class="info-cell">
            <h4>
              <b>Microsoft Research</b>
            </h4>
            <p>
              <a href="https://www.microsoft.com/en-us/research/people/yuegao/">Researcher</a>
            </p>
            <p>
              <em>Jun. 2021 - Mar. 2025</em>
            </p>
          </td>
        </table>
        <hr />
      </div>
      <div class="experience">
        <table align="center" border="0" cellpadding="0">
          <td class="image-cell">
            <img src="images/microsoft.svg" alt="microsoft" />
          </td>
          <td class="info-cell">
            <h4>
              <b>Microsoft Research</b>
            </h4>
            <p>
              <a href="https://www.microsoft.com/en-us/research/people/yuegao/">Research Intern</a>
            </p>
            <p>
              Award:<br />
              <span class="award-item">
                <b>Star of Tomorrow Award (Outstanding Intern)</b>
              </span>
            </p>
            <p>
              Mentors: <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ"><b>Fangyun Wei</b></a
              >, <a href="https://jianminbao.github.io/">Jianmin Bao</a>, <a href="http://www.dongchen.pro/">Dong Chen</a>
            </p>
            <p>
              <em>Jun. 2020 - Jun. 2021</em>
            </p>
          </td>
        </table>
        <hr />
      </div>
      <div class="experience">
        <table align="center" border="0" cellpadding="0">
          <td class="image-cell">
            <img src="images/apple.svg" alt="apple" />
          </td>
          <td class="info-cell">
            <h4>
              <b>Apple AI/ML</b>
            </h4>
            <p>Research Intern</p>
            <p>Mentors: <a href="https://www.linkedin.com/in/sam-li-513346b//">Sam Li</a>, Xiaoxu Wu, <a href="https://www.linkedin.com/in/shiyu-li-a43360a0/">Shiyu Li</a>.</p>
            <p>
              <em>Dec. 2019 - May 2020</em>
            </p>
          </td>
        </table>
      </div>
    </div>
    <br />

    <div class="container">
      <h2>Academic Service</h2>

      <div class="notes">
        <p><b>Reviewer:</b>
        <p><b>Conferences:</b> CVPR (Outstanding Reviewer 2025), ICCV, ECCV, SIGGRAPH, AAAI, ACM Multimedia, IJCAI</p>
        <p><b>Journals:</b> TOG, TPAMI, IJCV, ACM TOMM, IEEE TMM</p>
      </div>
    </div>
    <br />

    <div class="containersmall">
      <p>This website is modified from <a href="https://gkioxari.github.io/">Georgia Gkioxari</a>.</p>
    </div>
  </body>
</html>
